{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v04ltkfGS42t",
    "outputId": "543ec71d-990d-48fa-ff43-017ef164e39d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sziraqui/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'attention_decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5565346ed57d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mattention_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttentionDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'attention_decoder'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv3D, ZeroPadding3D, Conv2D, ZeroPadding2D\n",
    "from keras.layers.pooling import MaxPooling3D, MaxPooling2D\n",
    "from keras.layers.core import SpatialDropout3D, Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras import backend as K\n",
    "from attention_decoder import AttentionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANcPZiQHS420",
    "outputId": "4c187412-023c-47e0-9f57-c156b0f4fead"
   },
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "\n",
    "####################################### ONE ####################################### \n",
    "\n",
    "#conv1 pool1\n",
    "cnn.add(Conv2D(filters=96, kernel_size=(3,3), input_shape=(109,109,5), strides=(1,1), activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "#conv2 pool2\n",
    "cnn.add(Conv2D(filters=256, kernel_size=(3,3), strides=(2,2), activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "#conv3\n",
    "cnn.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu'))\n",
    "cnn.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "#conv4\n",
    "cnn.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "\n",
    "#conv5 pool5\n",
    "cnn.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "#fc6\n",
    "cnn.add(Flatten())\n",
    "\n",
    "#TimeDistributed\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(cnn, input_shape=(512,6))) #DOUBT :(\n",
    "\n",
    "#lstm\n",
    "model.add(LSTM(256, return_state=True))\n",
    "#Dense with softmax\n",
    "\n",
    "model.add(AttentionDecoder())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "WAS_Architecture_Sequential.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
