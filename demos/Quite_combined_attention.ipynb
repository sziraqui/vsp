{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quite_combined_attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "RqMHwKs-z8yE",
        "sm7KejnD1Ytq",
        "-wez3vxo2BRu",
        "bMF93FoN6Hbv",
        "CcdtrTAu_QOV",
        "spCedSg2_Blj",
        "G0g7u1xg7Ht2",
        "5Uo4EQoi8Z2G",
        "m7vh-NQ5_074",
        "unqVePmoADoT",
        "BVVPkYIqAOSS",
        "NouXGdg9Bobp",
        "iRj0auquHh-k",
        "f7NvOALlHqj9",
        "p902h8zgTxQF",
        "KPVvONx9T41s",
        "c5svWKYpUYhG",
        "uc7NFLTVj_-j"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "GdIzIq9byjHy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###############################################hkhkkkhh_hrhrrrhh########################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RqMHwKs-z8yE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Important Concepts"
      ]
    },
    {
      "metadata": {
        "id": "HS5Sq-XB0I-G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Eager Execution:  \n",
        "TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later. This makes it easy to get started with TensorFlow and debug models\n",
        "\n",
        "###Teacher Forcing:   \n",
        "During training time model receives ground truth output y(t) as input at time t+1. Maximum likelihood specifies that during training, rather than feeding the model’s own output back to itself, target values should specify what the correct output should be. \n",
        "####Train time: \n",
        "We feed the correct output y(t) (from teacher) drawn from the training set as input. \n",
        "####Test time: \n",
        "True output is not known. We approximate the correct output y(t) with the model’s output o(t) and feed the output back to the model."
      ]
    },
    {
      "metadata": {
        "id": "vMjl7u8N0yrc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Main Code"
      ]
    },
    {
      "metadata": {
        "id": "sm7KejnD1Ytq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ]
    },
    {
      "metadata": {
        "id": "IGtcFNI102q6",
        "colab_type": "code",
        "outputId": "23b4403b-b4d5-4e71-a709-3ffe3fcc6668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-2.0-preview\n",
        "import tensorflow as tf\n",
        "\n",
        "# We'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on during captioning\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn includes many helpful utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/12/03700d4ef679a35c1edfd278b154f6b668b159aaa0ce4bbc2a55e6128069/tf_nightly_2.0_preview-2.0.0.dev20190213-cp36-cp36m-manylinux1_x86_64.whl (77.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 77.9MB 583kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator-2.0-preview (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/19/f0e1994cc06637dcc44a90615656066261d5c35775f1f7707ae7d97470c7/tensorflow_estimator_2.0_preview-1.14.0.dev2019021400-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K    100% |████████████████████████████████| 358kB 21.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.32.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.6.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.0)\n",
            "Collecting google-pasta>=0.1.1 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/bf/15a18fa7017d17eff945704bc708153973a8e79f90855aa016dce6b61e70/google_pasta-0.1.3-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.9)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Collecting tb-nightly<1.14.0a0,>=1.13.0a0 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/c5/6683eb4a020e1e7403c8b88521261585fe8815f7866d7c7dbcf9a5be78d7/tb_nightly-1.13.0a20190214-py3-none-any.whl (3.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.2MB 12.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (0.14.1)\n",
            "Installing collected packages: tensorflow-estimator-2.0-preview, google-pasta, tb-nightly, tf-nightly-2.0-preview\n",
            "Successfully installed google-pasta-0.1.3 tb-nightly-1.13.0a20190214 tensorflow-estimator-2.0-preview-1.14.0.dev2019021400 tf-nightly-2.0-preview-2.0.0.dev20190213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vaG-QOMR0-GD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DpU-RMSt25qh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VYxzVbj1xel",
        "colab_type": "code",
        "outputId": "5d596bc1-82af-4f89-bc4c-f38ed0569442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "3qVHNBHYAvSL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install scikit-video\n",
        "import numpy as np\n",
        "import cv2\n",
        "import dlib\n",
        "import math\n",
        "import sys\n",
        "import pickle\n",
        "import argparse\n",
        "import os\n",
        "import skvideo.io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wez3vxo2BRu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Video Processing"
      ]
    },
    {
      "metadata": {
        "id": "bMF93FoN6Hbv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Download dlib files and grid tar files"
      ]
    },
    {
      "metadata": {
        "id": "xz2oa1-oYnkz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!wget http://spandh.dcs.shef.ac.uk/gridcorpus/examples/id2_vcd_swwp2s.mpg\n",
        "#!wget http://spandh.dcs.shef.ac.uk/gridcorpus/examples/id23_vcd_priazn.mpg\n",
        "#!wget http://spandh.dcs.shef.ac.uk/gridcorpus/examples/swwp2s.align\n",
        "#!wget http://spandh.dcs.shef.ac.uk/gridcorpus/examples/priazn.align"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9lHfrHm_Va7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9XcayDNL_a5O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://spandh.dcs.shef.ac.uk/gridcorpus/s1/video/s1.mpg_6000.part1.tar\n",
        "!tar -xvf s1.mpg_6000.part1.tar && rm -r s1.mpg_6000.part1.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CcdtrTAu_QOV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define dlib paths and set crop parameters"
      ]
    },
    {
      "metadata": {
        "id": "atMwl5Tn5fJP",
        "colab_type": "code",
        "outputId": "333e8aef-9228-48bd-a2c1-3b0d6eb9d225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#videos = ['id23_vcd_priazn.mpg', 'id2_vcd_swwp2s.mpg']\n",
        "predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(predictor_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-video in /usr/local/lib/python3.6/dist-packages (1.1.11)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from scikit-video) (4.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.14.6)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->scikit-video) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "avM7dC2o-0wE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "width_crop_max = 100\n",
        "height_crop_max = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "spCedSg2_Blj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lip Extract"
      ]
    },
    {
      "metadata": {
        "id": "-l6pcGtC-1YX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "video_dir='mpg_6000/'\n",
        "videos = [video_dir + i for i in os.listdir(video_dir)]\n",
        "videos=sorted(videos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SoHhIbwK56yr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_mouth_crops(video):\n",
        "  global width_crop_max,height_crop_max\n",
        "\n",
        "  inputparameters = {}\n",
        "  outputparameters = {}\n",
        "  reader = skvideo.io.FFmpegReader(video, inputdict=inputparameters, outputdict=outputparameters)\n",
        "  video_shape = reader.getShape()\n",
        "  (num_frames, h, w, c) = video_shape\n",
        "\n",
        "  max_counter = 150\n",
        "  total_num_frames = int(video_shape[0])\n",
        "  num_frames = min(total_num_frames,max_counter)\n",
        "  counter = 0\n",
        "\n",
        "  out_frame=[]\n",
        "\n",
        "\n",
        "\n",
        "  # Loop over all frames.\n",
        "  for frame in reader.nextFrame():\n",
        "    #print('frame_shape:', frame.shape)\n",
        "    # Process the video and extract the frames up to a certain number and then stop processing.\n",
        "    if counter > num_frames:\n",
        "      break\n",
        "    # Detection of the frame\n",
        "    detections = detector(frame, 1)\n",
        "    # 20 mark for mouth\n",
        "    marks = np.zeros((2, 20))\n",
        "    # All unnormalized face features.\n",
        "\n",
        "    Features_Abnormal = np.zeros((190, 1))\n",
        "\n",
        "    # If the face is detected.\n",
        "    #print(len(detections))\n",
        "\n",
        "    if len(detections) > 0:\n",
        "      # Shape of the face.\n",
        "      shape = predictor(frame, detections[0]) #single face considered \n",
        "      co = 0\n",
        "      # Specific for the mouth.\n",
        "      for ii in range(48, 68):\n",
        "        \"\"\"\n",
        "        This for loop is going over all mouth-related features.\n",
        "        X and Y coordinates are extracted and stored separately.\n",
        "        \"\"\"\n",
        "        X = shape.part(ii)\n",
        "        A = (X.x, X.y)\n",
        "        marks[0, co] = X.x\n",
        "        marks[1, co] = X.y\n",
        "        co += 1\n",
        "\n",
        "      # Get the extreme points(top-left & bottom-right)\n",
        "      X_left, Y_left, X_right, Y_right = [int(np.amin(marks, axis=1)[0]), int(np.amin(marks, axis=1)[1]),\n",
        "                                          int(np.amax(marks, axis=1)[0]),\n",
        "                                          int(np.amax(marks, axis=1)[1])]\n",
        "      # Find the center of the mouth.\n",
        "      X_center = (X_left + X_right) / 2.0\n",
        "      Y_center = (Y_left + Y_right) / 2.0\n",
        "\n",
        "      # Make a boarder for cropping.\n",
        "      border = 15\n",
        "      X_left_new = X_left - border\n",
        "      Y_left_new = Y_left - border\n",
        "      X_right_new = X_right + border\n",
        "      Y_right_new = Y_right + border\n",
        "\n",
        "      # Width and height for cropping(before and after considering the border).\n",
        "      width_new = X_right_new - X_left_new\n",
        "      height_new = Y_right_new - Y_left_new\n",
        "      width_current = X_right - X_left\n",
        "      height_current = Y_right - Y_left\n",
        "\n",
        "\n",
        "      # Determine the cropping rectangle dimensions(the main purpose is to have a fixed area).\n",
        "      if width_crop_max == 0 and height_crop_max == 0:\n",
        "        width_crop_max = width_new\n",
        "        height_crop_max = height_new\n",
        "      else:\n",
        "        width_crop_max += 1.5 * np.maximum(width_current - width_crop_max, 0)\n",
        "        height_crop_max += 1.5 * np.maximum(height_current - height_crop_max, 0)\n",
        "\n",
        "      # # # Uncomment if the lip area is desired to be rectangular # # # #\n",
        "      #########################################################\n",
        "      # Find the cropping points(top-left and bottom-right).\n",
        "      X_left_crop = int(X_center - width_crop_max / 2.0)\n",
        "      X_right_crop = int(X_center + width_crop_max / 2.0)\n",
        "      Y_left_crop = int(Y_center - height_crop_max / 2.0)\n",
        "      Y_right_crop = int(Y_center + height_crop_max / 2.0)\n",
        "\n",
        "\n",
        "\n",
        "      if X_left_crop >= 0 and Y_left_crop >= 0 and X_right_crop < w and Y_right_crop < h:\n",
        "        mouth = frame[Y_left_crop:Y_right_crop, X_left_crop:X_right_crop, :]\n",
        "        #cv2.imwrite(mouth_destination_path + '/' + 'frame' + '_' + str(counter) + '.png', mouth)\n",
        "        out_frame.append(mouth.T)\n",
        "        counter += 1\n",
        "      else:\n",
        "        cv2.putText(frame, 'NA. ', (30, 30), font, 1, (0, 255, 255), 2) #Check in LRS\n",
        "\n",
        "    else:\n",
        "      cv2.putText(frame, 'NA. ', (30, 30), font, 1, (0, 0, 255), 2)#Check in LRS\n",
        "    \n",
        "  out_frame=np.asarray(out_frame).reshape(-1,100,50,3)\n",
        "    \n",
        "  return out_frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G0g7u1xg7Ht2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Process frames"
      ]
    },
    {
      "metadata": {
        "id": "J7wt6x8u6ARg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train=[]\n",
        "for i,v in enumerate(videos):\n",
        "  if i>9:\n",
        "    break\n",
        "  out_frame=make_mouth_crops(v)\n",
        "  X_train.append(out_frame)\n",
        "  print(\"Video \",i,\" : \",out_frame.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xKv8Lyve_oZB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = np.asarray(X_train)\n",
        "X_train = X_train.astype('float32')\n",
        "X_train /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Uo4EQoi8Z2G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Text Processing"
      ]
    },
    {
      "metadata": {
        "id": "m7vh-NQ5_074",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get transcript tars"
      ]
    },
    {
      "metadata": {
        "id": "bHoljAKN_vp0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://spandh.dcs.shef.ac.uk/gridcorpus/s1/align/s1.tar\n",
        "!tar -xvf s1.tar && rm -r s1.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "slg_mBIk8b5z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#files=['swwp2s.align','priazn.align']\n",
        "def prep_text(files):\n",
        "  sent=[]\n",
        "  for file in files:\n",
        "    f = open(file, \"r\")\n",
        "    dummy=[line.split()[-1] for line in f] #-1 coz that's the word we want, 0 and 1 have timestamps\n",
        "    dummy[0]='<start>'\n",
        "    dummy[-1]='<end>'\n",
        "    sent.append(dummy)\n",
        "  return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KW6uBH-6_9c5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_dir='align/'\n",
        "files = [text_dir + i for i in os.listdir(text_dir)]\n",
        "files=sorted(files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "unqVePmoADoT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare transcripts"
      ]
    },
    {
      "metadata": {
        "id": "luDvPNj0-C-J",
        "colab_type": "code",
        "outputId": "9aca0dbb-ec28-4b4e-f6e2-c1a04b3dc42c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "sent = prep_text(files)\n",
        "#sent"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<start>', 'set', 'white', 'with', 'p', 'two', 'soon', '<end>'],\n",
              " ['<start>', 'place', 'red', 'in', 'a', 'zero', 'now', '<end>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "BVVPkYIqAOSS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenize"
      ]
    },
    {
      "metadata": {
        "id": "CREtVDEU-m88",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#CHANGE: Have used keras instead of tf.keras\n",
        "tokenizer = Tokenizer(oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(sent)\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "#tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n5p5zJ8x-udP",
        "colab_type": "code",
        "outputId": "d11facf2-2ef6-49b1-a3d3-912cee01cfcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "sseq = tokenizer.texts_to_sequences(sent)\n",
        "#sseq"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 4, 5, 6, 7, 8, 9, 3], [2, 10, 11, 12, 13, 14, 15, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "rrCm22u0AZTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_input_data=pad_sequences(sseq, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gy3-6wyMAaGQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_input_data=decoder_input_data[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NouXGdg9Bobp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Making the Dataset"
      ]
    },
    {
      "metadata": {
        "id": "z7SXlfupBqkh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2 #2 videos\n",
        "BUFFER_SIZE = len(X_train)\n",
        "embedding_dim = 8\n",
        "units = 256\n",
        "vocab_size = len(tokenizer.word_index)\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jtEch8NJG2hy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, decoder_input_data)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iRj0auquHh-k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Model "
      ]
    },
    {
      "metadata": {
        "id": "f7NvOALlHqj9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###GRUs/LSTMs/BahdanauAttention"
      ]
    },
    {
      "metadata": {
        "id": "PIQa9gC3HmkD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#NOTE: GRU outputs state = [h,c] But, LSTM doesn't give this list, instead separately h,c\n",
        "def gru(units):\n",
        "  # If you have a GPU, we recommend using the CuDNNGRU layer (it provides a \n",
        "  # significant speedup).\n",
        "  if tf.test.is_gpu_available():\n",
        "    return tf.keras.layers.CuDNNGRU(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "  else:\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')\n",
        "  \n",
        "def lstm(units):\n",
        "  # If you have a GPU, we recommend using the CuDNNLSTM layer (it provides a \n",
        "  # significant speedup).\n",
        "  if tf.test.is_gpu_available():\n",
        "    return tf.keras.layers.CuDNNLSTM(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "  else:\n",
        "    return tf.keras.layers.LSTM(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q32dG7MAIRIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "    \n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "    \n",
        "    # score shape == (batch_size, 64, hidden_size)\n",
        "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "    \n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "    \n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    \n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p902h8zgTxQF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Encoder-Decoder Architecture"
      ]
    },
    {
      "metadata": {
        "id": "PfxsKksnI9mZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        \n",
        "        #NOTE: Define as many times as you want to use. Thus, there will be REDUNDANCY\n",
        "        self.zero1 = tf.keras.layers.ZeroPadding3D(padding=(1,2,2))\n",
        "        self.conv1 = tf.keras.layers.Conv3D(filters=32, kernel_size=(3,5,5), strides=(1,2,2), kernel_initializer='he_normal')\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        #activation defined later\n",
        "        self.spd1 = tf.keras.layers.SpatialDropout3D(0.5)\n",
        "        self.maxp1 = tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2))\n",
        "        #activation defined later\n",
        "        self.zero2 = tf.keras.layers.ZeroPadding3D(padding=(1,2,2))\n",
        "        self.conv2 = tf.keras.layers.Conv3D(filters=64, kernel_size=(3,5,5), strides=(1,1,1), kernel_initializer='he_normal')\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        #activation defined later\n",
        "        self.spd2 = tf.keras.layers.SpatialDropout3D(0.5)\n",
        "        self.maxp2 = tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2))\n",
        "        #activation defined later\n",
        "        self.zero3 = tf.keras.layers.ZeroPadding3D(padding=(1,1,1))\n",
        "        self.conv3 = tf.keras.layers.Conv3D(filters=96, kernel_size=(3,3,3), strides=(1,1,1), kernel_initializer='he_normal')\n",
        "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
        "        #activation defined later\n",
        "        self.spd3 = tf.keras.layers.SpatialDropout3D(0.5)\n",
        "        self.maxp3 = tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2))\n",
        "        \n",
        "        self.td=tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
        "        self.gru=gru(units=256)\n",
        "        \n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        zero1 = self.zero1(x)\n",
        "        conv1 = self.conv1(zero1)\n",
        "        batc1 = self.bn1(conv1)\n",
        "        actv1 = tf.nn.relu(batc1)\n",
        "        drop1 = self.spd1(actv1)\n",
        "        maxp1 = self.maxp1(drop1)\n",
        "\n",
        "        zero2 = self.zero2(maxp1)\n",
        "        conv2 = self.conv2(zero2)\n",
        "        batc2 = self.bn2(conv2)\n",
        "        actv2 = tf.nn.relu(batc2)\n",
        "        drop2 = self.spd2(actv2)\n",
        "        maxp2 = self.maxp2(drop2)\n",
        "\n",
        "        zero3 = self.zero3(maxp2)\n",
        "        conv3 = self.conv3(zero3)\n",
        "        batc3 = self.bn3(conv3)\n",
        "        actv3 = tf.nn.relu(batc3)\n",
        "        drop3 = self.spd3(actv3)\n",
        "        maxp3 = self.maxp3(drop3)\n",
        "\n",
        "        resh1 = self.td(maxp3)\n",
        "        output, state=self.gru(resh1,initial_state = hidden)\n",
        "        \n",
        "        #output, state_h, state_c=self.lstm(resh1,initial_state = hidden)\n",
        "        #state = [state_h, state_c]\n",
        "        \n",
        "        return output, state\n",
        "      \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PU9PrHP0M0iC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.units = units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = gru(self.units)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "    \n",
        "    #features = enc_output\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "    \n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "    \n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    \n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "    #output, state_h, state_c = self.lstm(x)\n",
        "    #state = [state_h, state_c]\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc(output)\n",
        "    \n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CdGw_PaESuzL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = Encoder(units, BATCH_SIZE)\n",
        "decoder = Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KPVvONx9T41s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Optimizer & Loss Function"
      ]
    },
    {
      "metadata": {
        "id": "TUgP5cvxBD3d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#can use this loss function too\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# We are masking the loss calculated for padding\n",
        "def loss_function(real, pred):\n",
        "    mask = 1 - np.equal(real, 0)\n",
        "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ea6gYQ60TVSx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#this loss function requires tf.nightly otherwise it will throw an error\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c5svWKYpUYhG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Training"
      ]
    },
    {
      "metadata": {
        "id": "6tfUtLT8UZ2F",
        "colab_type": "code",
        "outputId": "84df383a-ff5a-4799-9974-40c7b2bdbfb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4304
        }
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "      \n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            features, enc_hidden = encoder(img_tensor, hidden)\n",
        "            dec_hidden = enc_hidden\n",
        "            dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "            \n",
        "            for i in range(1, target.shape[1]):\n",
        "                # passing the features through the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, features, enc_hidden)\n",
        "\n",
        "                loss += loss_function(target[:, i], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(target[:, i], 1)\n",
        "        \n",
        "        total_loss += (loss / int(target.shape[1]))\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables) \n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables)) #tf.train.get_or_create_global_step())\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, \n",
        "                                                          batch, \n",
        "                                                          loss.numpy() / int(target.shape[1])))\n",
        "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, \n",
        "                                         total_loss/N_BATCH))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0209 10:45:30.147437 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.4739\n",
            "Epoch 1 Loss 2.473852\n",
            "Time taken for 1 epoch 6.058124780654907 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:45:35.757418 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2 Batch 0 Loss 2.4022\n",
            "Epoch 2 Loss 2.402231\n",
            "Time taken for 1 epoch 5.515782833099365 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:45:41.256341 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3 Batch 0 Loss 2.3019\n",
            "Epoch 3 Loss 2.301866\n",
            "Time taken for 1 epoch 5.497985601425171 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:45:46.780691 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4 Batch 0 Loss 2.2442\n",
            "Epoch 4 Loss 2.244217\n",
            "Time taken for 1 epoch 5.523183584213257 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:45:52.242707 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5 Batch 0 Loss 2.2053\n",
            "Epoch 5 Loss 2.205253\n",
            "Time taken for 1 epoch 5.461856842041016 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:45:57.771348 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6 Batch 0 Loss 2.1584\n",
            "Epoch 6 Loss 2.158395\n",
            "Time taken for 1 epoch 5.531104326248169 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:46:03.246692 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7 Batch 0 Loss 2.0911\n",
            "Epoch 7 Loss 2.091056\n",
            "Time taken for 1 epoch 5.471453428268433 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:46:08.760835 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8 Batch 0 Loss 2.0198\n",
            "Epoch 8 Loss 2.019826\n",
            "Time taken for 1 epoch 5.514042854309082 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:46:14.217899 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9 Batch 0 Loss 1.9575\n",
            "Epoch 9 Loss 1.957508\n",
            "Time taken for 1 epoch 5.4564526081085205 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:46:19.756787 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10 Batch 0 Loss 1.8883\n",
            "Epoch 10 Loss 1.888324\n",
            "Time taken for 1 epoch 5.540209531784058 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:46:25.231121 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11 Batch 0 Loss 1.8309\n",
            "Epoch 11 Loss 1.830918\n",
            "Time taken for 1 epoch 5.473221302032471 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:46:30.772515 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12 Batch 0 Loss 1.7986\n",
            "Epoch 12 Loss 1.798552\n",
            "Time taken for 1 epoch 5.545332670211792 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:46:36.283198 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13 Batch 0 Loss 1.7693\n",
            "Epoch 13 Loss 1.769297\n",
            "Time taken for 1 epoch 5.506338596343994 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:46:41.856865 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14 Batch 0 Loss 1.7477\n",
            "Epoch 14 Loss 1.747740\n",
            "Time taken for 1 epoch 5.575722694396973 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:46:47.347172 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15 Batch 0 Loss 1.7357\n",
            "Epoch 15 Loss 1.735660\n",
            "Time taken for 1 epoch 5.488245964050293 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:46:52.948572 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16 Batch 0 Loss 1.7259\n",
            "Epoch 16 Loss 1.725922\n",
            "Time taken for 1 epoch 5.600298881530762 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:46:58.527939 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17 Batch 0 Loss 1.7177\n",
            "Epoch 17 Loss 1.717720\n",
            "Time taken for 1 epoch 5.579325914382935 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:47:04.187880 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18 Batch 0 Loss 1.7114\n",
            "Epoch 18 Loss 1.711398\n",
            "Time taken for 1 epoch 5.664720773696899 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:47:09.913937 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19 Batch 0 Loss 1.7068\n",
            "Epoch 19 Loss 1.706806\n",
            "Time taken for 1 epoch 5.720273017883301 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:47:15.516823 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20 Batch 0 Loss 1.7037\n",
            "Epoch 20 Loss 1.703653\n",
            "Time taken for 1 epoch 5.604251384735107 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:47:21.049626 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21 Batch 0 Loss 1.7010\n",
            "Epoch 21 Loss 1.701017\n",
            "Time taken for 1 epoch 5.53350305557251 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:47:26.540013 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22 Batch 0 Loss 1.6984\n",
            "Epoch 22 Loss 1.698382\n",
            "Time taken for 1 epoch 5.48895788192749 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:47:32.090540 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23 Batch 0 Loss 1.6965\n",
            "Epoch 23 Loss 1.696491\n",
            "Time taken for 1 epoch 5.5523576736450195 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:47:37.561939 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24 Batch 0 Loss 1.6946\n",
            "Epoch 24 Loss 1.694617\n",
            "Time taken for 1 epoch 5.4703381061553955 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:47:43.116010 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25 Batch 0 Loss 1.6930\n",
            "Epoch 25 Loss 1.692979\n",
            "Time taken for 1 epoch 5.552788257598877 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:47:48.617761 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26 Batch 0 Loss 1.6913\n",
            "Epoch 26 Loss 1.691288\n",
            "Time taken for 1 epoch 5.501406192779541 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:47:54.158643 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27 Batch 0 Loss 1.6898\n",
            "Epoch 27 Loss 1.689796\n",
            "Time taken for 1 epoch 5.5419602394104 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:47:59.676058 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28 Batch 0 Loss 1.6883\n",
            "Epoch 28 Loss 1.688259\n",
            "Time taken for 1 epoch 5.5167529582977295 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:48:05.216810 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29 Batch 0 Loss 1.6867\n",
            "Epoch 29 Loss 1.686710\n",
            "Time taken for 1 epoch 5.539386510848999 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:48:10.770023 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30 Batch 0 Loss 1.6851\n",
            "Epoch 30 Loss 1.685102\n",
            "Time taken for 1 epoch 5.552064418792725 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:48:16.241148 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31 Batch 0 Loss 1.6835\n",
            "Epoch 31 Loss 1.683464\n",
            "Time taken for 1 epoch 5.472560882568359 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:48:21.763255 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32 Batch 0 Loss 1.6819\n",
            "Epoch 32 Loss 1.681939\n",
            "Time taken for 1 epoch 5.521003246307373 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:48:27.252296 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33 Batch 0 Loss 1.6803\n",
            "Epoch 33 Loss 1.680325\n",
            "Time taken for 1 epoch 5.48955512046814 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:48:32.782943 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34 Batch 0 Loss 1.6784\n",
            "Epoch 34 Loss 1.678360\n",
            "Time taken for 1 epoch 5.529613494873047 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:48:38.394801 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35 Batch 0 Loss 1.6764\n",
            "Epoch 35 Loss 1.676395\n",
            "Time taken for 1 epoch 5.612481355667114 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:48:43.926583 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36 Batch 0 Loss 1.6745\n",
            "Epoch 36 Loss 1.674453\n",
            "Time taken for 1 epoch 5.532297372817993 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:48:49.403324 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37 Batch 0 Loss 1.6724\n",
            "Epoch 37 Loss 1.672429\n",
            "Time taken for 1 epoch 5.474032640457153 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:48:54.929293 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38 Batch 0 Loss 1.6701\n",
            "Epoch 38 Loss 1.670072\n",
            "Time taken for 1 epoch 5.5271313190460205 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:49:00.384760 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39 Batch 0 Loss 1.6675\n",
            "Epoch 39 Loss 1.667522\n",
            "Time taken for 1 epoch 5.455562591552734 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:49:05.868659 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40 Batch 0 Loss 1.6649\n",
            "Epoch 40 Loss 1.664878\n",
            "Time taken for 1 epoch 5.483682870864868 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:49:11.321026 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41 Batch 0 Loss 1.6629\n",
            "Epoch 41 Loss 1.662864\n",
            "Time taken for 1 epoch 5.451425075531006 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:49:16.841104 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42 Batch 0 Loss 1.6593\n",
            "Epoch 42 Loss 1.659288\n",
            "Time taken for 1 epoch 5.52110743522644 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:49:22.295638 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43 Batch 0 Loss 1.6562\n",
            "Epoch 43 Loss 1.656229\n",
            "Time taken for 1 epoch 5.453531265258789 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:49:27.879359 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44 Batch 0 Loss 1.6531\n",
            "Epoch 44 Loss 1.653126\n",
            "Time taken for 1 epoch 5.583830118179321 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:49:33.426772 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45 Batch 0 Loss 1.6495\n",
            "Epoch 45 Loss 1.649472\n",
            "Time taken for 1 epoch 5.5474913120269775 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:49:38.988225 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46 Batch 0 Loss 1.6458\n",
            "Epoch 46 Loss 1.645779\n",
            "Time taken for 1 epoch 5.560024738311768 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:49:44.645450 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47 Batch 0 Loss 1.6420\n",
            "Epoch 47 Loss 1.642004\n",
            "Time taken for 1 epoch 5.659403085708618 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:49:50.290608 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48 Batch 0 Loss 1.6376\n",
            "Epoch 48 Loss 1.637624\n",
            "Time taken for 1 epoch 5.643473386764526 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:49:55.769579 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49 Batch 0 Loss 1.6330\n",
            "Epoch 49 Loss 1.633048\n",
            "Time taken for 1 epoch 5.478773355484009 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0209 10:50:01.317166 139727270086528 optimizer_v2.py:927] Gradients does not exist for variables ['encoder/batch_normalization_v2/moving_mean:0', 'encoder/batch_normalization_v2/moving_variance:0', 'encoder/batch_normalization_v2_1/moving_mean:0', 'encoder/batch_normalization_v2_1/moving_variance:0', 'encoder/batch_normalization_v2_2/moving_mean:0', 'encoder/batch_normalization_v2_2/moving_variance:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50 Batch 0 Loss 1.6286\n",
            "Epoch 50 Loss 1.628558\n",
            "Time taken for 1 epoch 5.547411680221558 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uc7NFLTVj_-j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Evaluation & Prediction"
      ]
    },
    {
      "metadata": {
        "id": "ENYM7zJlkQly",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(image):\n",
        "     \n",
        "    result = ''\n",
        "    image = tf.convert_to_tensor(image)\n",
        "    inputs = tf.expand_dims(image, 0)\n",
        "   \n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "    features, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        " \n",
        "    for i in range(8):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result = result + tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qohfAZwXpPUq",
        "colab_type": "code",
        "outputId": "568eb2fa-d7c7-4c06-f958-8cb2747afb69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "evaluate(X_train[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'place red in a now <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "tWKZXqJW4YVe",
        "colab_type": "code",
        "outputId": "bf481bae-56c9-4c3c-dd50-e2a33ecf4f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "evaluate(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'set white with p two <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "mYwJNs7z4aNh",
        "colab_type": "code",
        "outputId": "19b3c847-9723-4cc1-b9c8-377563578d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "sent"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<start>', 'set', 'white', 'with', 'p', 'two', 'soon', '<end>'],\n",
              " ['<start>', 'place', 'red', 'in', 'a', 'zero', 'now', '<end>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}