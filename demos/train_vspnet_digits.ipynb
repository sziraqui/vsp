{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VSPNet layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from modules.lipreading import VSPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.metrics import vspnet_loss as loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import h5py\n",
    "import time\n",
    "from os import path\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from modules.framestream import VisemeStream\n",
    "from modules.generators import GeneratorInterface\n",
    "from modules.utils import parse_config, get_sample_ids, load_tokenizer, Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_digits_tokenizer():\n",
    "    tokenizer = Tokenizer(oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "    sp_tokens = ['<pad>','<start>', '<end>', '<unk>']\n",
    "    digits = list('0123456789')\n",
    "    tokenizer.fit_on_texts(sp_tokens + digits)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = build_digits_tokenizer()\n",
    "with open('../config/vsp-digits-tokenizer.json', 'w') as f:\n",
    "    print(tokenizer.to_json(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Creates batches from raw videos and transcripts on-the-fly\n",
    "    VisemeStream is responsible for viseme extraction\n",
    "'''\n",
    "class OnlineDigitData(GeneratorInterface):\n",
    "\n",
    "    def __init__(self, config, max_label_len=4):\n",
    "        GeneratorInterface.__init__(self)\n",
    "        self.config = config\n",
    "        self.inputShape = (config['frame_length'], config['frame_height'], config['frame_width'], 3)\n",
    "        self.seqLen = max_label_len\n",
    "        '''\n",
    "            videoDir:\n",
    "            eg. video_list[0]: /home/sziraqui/vsp-dev/datasets/GRID/videos/s1/bbac9n.mpg\n",
    "            Then videoDir: /home/sziraqui/vsp-dev/datasets/GRID/videos\n",
    "\n",
    "            Likewise for textDir\n",
    "            eg. text_list[0]: /home/sziraqui/vsp-dev/datasets/GRID/transcripts/s1/bbac9n.align\n",
    "            Then textDir: /home/sziraqui/vsp-dev/datasets/GRID/transcripts\n",
    "        '''\n",
    "        self.videoDir = path.dirname(path.dirname(config['video_list'][0]))\n",
    "        self.videoExt = path.basename(config['video_list'][0]).split('.')[-1] # eg 'mpg'\n",
    "        \n",
    "        self.cache = config['cache_dir']\n",
    "        \n",
    "        self.ids = get_sample_ids(config['video_list'])\n",
    "        np.random.shuffle(self.ids)\n",
    "\n",
    "        self.sampleIndex = 0 # current sample index in self.ids\n",
    "        self.tokenizer = load_tokenizer(config['tokenizer'])\n",
    "        self.vs = VisemeStream(params=config)\n",
    "        \n",
    "\n",
    "    def next_data(self):\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                print('Processing file:', self.ids[self.sampleIndex])\n",
    "                visemes = self.load_from_cache(self.ids[self.sampleIndex])\n",
    "                if visemes is None:\n",
    "                    visemes = self.load_from_disk(self.ids[self.sampleIndex ])\n",
    "                    self.save_to_cache(self.ids[self.sampleIndex], visemes)\n",
    "                    \n",
    "                word = self.load_transcript(self.ids[self.sampleIndex])\n",
    "                seq = self.tokenizer.texts_to_sequences([word])[0]\n",
    "                if visemes[:25].shape[0] == 25:\n",
    "                    yield visemes[:25], seq\n",
    "            except KeyboardInterrupt as e:\n",
    "                Log.error(repr(e))\n",
    "                sys.exit(1)\n",
    "            except Exception as e:\n",
    "                Log.error(repr(e) + ' Error processing sample ' + self.ids[self.sampleIndex])\n",
    "                continue\n",
    "            self.sampleIndex = (self.sampleIndex + 1)%len(self.ids)\n",
    "        \n",
    "    \n",
    "    def next_batch(self, batchSize):\n",
    "        while True:\n",
    "            X = np.zeros((batchSize,) + (self.inputShape))\n",
    "            sequences = [None]*batachSize\n",
    "            \n",
    "            i = 0 # current batch index\n",
    "            for visemes, seq in self.next_data():\n",
    "                X[i] = visemes\n",
    "                sequences[i] = seq\n",
    "                i+=1\n",
    "                if i > batchSize:\n",
    "                    break\n",
    "            Y = pad_sequences(sequences, value=self.tokenizer.word_index['<pad>'], maxlen=self.seqLen, \n",
    "                            dtype='uint8', padding='post', truncating='post')\n",
    "            X /= 255\n",
    "            \n",
    "            yield X, Y\n",
    "            del X\n",
    "            del Y\n",
    "\n",
    "   \n",
    "    def load_from_cache(self, id):\n",
    "        try:\n",
    "            with h5py.File(path.join(self.cache, id + '.h5')) as f:\n",
    "                print('Found in cache')\n",
    "                viseme = f[\"features\"][:]\n",
    "                if viseme.shape[1:] != self.inputShape[1:]:\n",
    "                    print('Incorrect shape {} in cached data. Skipping'.format(viseme.shape))\n",
    "                    return None\n",
    "                else:\n",
    "                    print('Using cached data')\n",
    "                    return viseme\n",
    "        except (OSError, KeyError):\n",
    "            return None\n",
    "    \n",
    "\n",
    "    def load_from_disk(self, id):\n",
    "        print('Building viseme')\n",
    "        self.vs.set_source(path.join(self.videoDir, id + '.' + self.videoExt))\n",
    "        visemes = []\n",
    "        frame = self.vs.next_frame()\n",
    "        while frame is not None:\n",
    "            visemes.append(frame)\n",
    "            frame = self.vs.next_frame()\n",
    "        return np.array(visemes)\n",
    "\n",
    "    \n",
    "    def load_transcript(self, id):\n",
    "        digit = id.split('_')[-1].split('.')[0]\n",
    "        return ['<start>'] + [digit] + ['<end>']\n",
    "    \n",
    "    \n",
    "    def save_to_cache(self, id, visemes):\n",
    "        filename = path.join(self.cache, id + '.h5')\n",
    "        os.makedirs(path.dirname(filename), exist_ok=True)\n",
    "        \n",
    "        with h5py.File(filename, 'w') as f:\n",
    "            f.create_dataset(\"features\", data=visemes, dtype='uint8', compression=\"gzip\", compression_opts=4)\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = parse_config('../config/config-train-vspnet-digits.json')\n",
    "tokenizer = build_digits_tokenizer()\n",
    "gen = OnlineDigitData(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(gen.next_data, (tf.float64, tf.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2\n",
    "BATCH_SIZE = config['batch_size']\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 32\n",
    "units = 256\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "max_length_targ= gen.seqLen-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.layers import Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '../weights'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell \n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n",
    "train_accuracy_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: VSP_DIGITS/Shriya_3\n",
      "Found in cache\n",
      "Using cached data\n",
      "Processing file: VSP_DIGITS/Shriya_0\n",
      "Found in cache\n",
      "Using cached data\n",
      "Processing file: VSP_DIGITS/tarang_7\n",
      "Found in cache\n",
      "Incorrect shape (0,) in cached data. Skipping\n",
      "Building viseme\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Could not find valid device for node.\nNode: {{node MaxPool3D}} = MaxPool3D[T=DT_DOUBLE, data_format=\"NDHWC\", ksize=[1, 1, 2, 2, 1], padding=\"VALID\", strides=[1, 1, 2, 2, 1]](dummy_input)\nAll kernels registered for op MaxPool3D :\n  device='XLA_CPU'; T in [DT_FLOAT, DT_HALF]\n  device='CPU'; T in [DT_FLOAT]\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_HALF]\n [Op:MaxPool3D]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b4f3f2647da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpred_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyApps/miniconda3/envs/vsp-cpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/vsp-dev/modules/layers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mactv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mdrop1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspd1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mmaxp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mzero2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyApps/miniconda3/envs/vsp-cpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyApps/miniconda3/envs/vsp-cpu/lib/python3.6/site-packages/tensorflow/python/keras/layers/pooling.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpool_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         padding=self.padding.upper())\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_first'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyApps/miniconda3/envs/vsp-cpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool3d\u001b[0;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   4773\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4774\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4775\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyApps/miniconda3/envs/vsp-cpu/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Could not find valid device for node.\nNode: {{node MaxPool3D}} = MaxPool3D[T=DT_DOUBLE, data_format=\"NDHWC\", ksize=[1, 1, 2, 2, 1], padding=\"VALID\", strides=[1, 1, 2, 2, 1]](dummy_input)\nAll kernels registered for op MaxPool3D :\n  device='XLA_CPU'; T in [DT_FLOAT, DT_HALF]\n  device='CPU'; T in [DT_FLOAT]\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_HALF]\n [Op:MaxPool3D]"
     ]
    }
   ],
   "source": [
    "EPOCHS = config['epochs']\n",
    "dataset = tf.data.Dataset.batch(dataset, batch_size=BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    epoch_accuracy = tf.contrib.eager.metrics.Accuracy()#accuracy #Change here for without Nightly\n",
    "    \n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        pred_list=[]#accuracy\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(img_tensor, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, target.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                pred_list.append(tf.argmax(predictions, axis=1, output_type=tf.int32))#accuracy\n",
    "                \n",
    "                loss += loss_function(target[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(target[:, t], 1)\n",
    "        \n",
    "        total_loss += (loss / int(target.shape[1]))\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables) \n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))#, tf.train.get_or_create_global_step()\n",
    "\n",
    "        epoch_accuracy(np.asarray(pred_list).T, target[:,1:]) #accuracy\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, \n",
    "                                                          batch, \n",
    "                                                          loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / N_BATCH)\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "    \n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    #if (epoch + 1) % 5 == 0:\n",
    "      #checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print ('Epoch {} Loss {:.6f}, Accuracy: {:.3%}'.format(epoch + 1, \n",
    "                                                           total_loss/N_BATCH,\n",
    "                                                           epoch_accuracy.result()))#accuracy\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: VSP_DIGITS/Shriya_5\n",
      "Found in cache\n",
      "Using cached data\n",
      "Processing file: VSP_DIGITS/akarshan_9\n",
      "Found in cache\n",
      "Using cached data\n",
      "Processing file: VSP_DIGITS/sarfaraz_9\n",
      "Found in cache\n",
      "Incorrect shape (0,) in cached data. Skipping\n",
      "Building viseme\n",
      "batch 0\n",
      "img_tensor (2, 25, 50, 100, 3)\n",
      "target tf.Tensor(\n",
      "[[ 3 11  4]\n",
      " [ 3 15  4]], shape=(2, 3), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batchData = tf.data.Dataset.batch(dataset, batch_size=BATCH_SIZE)\n",
    "count = 0\n",
    "for (batch, (img_tensor, target)) in enumerate(batchData):\n",
    "    print('batch', batch)\n",
    "    print('img_tensor', img_tensor.shape)\n",
    "    print('target', target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
